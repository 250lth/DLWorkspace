{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "\n",
    "Gets to 99.25% test accuracy after 28 epochs\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 200\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# create a grid of 3x3 images\n",
    "for i in range(0, 9):\n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "    pyplot.imshow(x_train[i], cmap=pyplot.get_cmap('gray'))\n",
    "    \n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "nb_train_samples = x_train.shape[0]\n",
    "nb_test_samples = x_test.shape[0]\n",
    "\n",
    "# define data preparation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255.,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2 )\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255.)\n",
    "\n",
    "# fit parameters from data\n",
    "train_datagen.fit(x_train)\n",
    "test_datagen.fit(x_test)\n",
    "# configure batch size and retrieve one batch of images\n",
    "for x_batch, y_batch in train_datagen.flow(x_train, y_train, batch_size=9):\n",
    "    # create a grid of 3x3 images\n",
    "    for i in range(0, 9):\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        pyplot.imshow(x_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "46/46 [==============================] - 19s 406ms/step - loss: 1.8957 - acc: 0.3472 - val_loss: 0.8277 - val_acc: 0.8019\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 15s 324ms/step - loss: 1.2577 - acc: 0.5834 - val_loss: 0.6444 - val_acc: 0.7977\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 15s 321ms/step - loss: 1.0545 - acc: 0.6498 - val_loss: 0.5966 - val_acc: 0.8092\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 15s 328ms/step - loss: 0.9619 - acc: 0.6858 - val_loss: 0.5602 - val_acc: 0.8186\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 16s 349ms/step - loss: 0.8991 - acc: 0.7080 - val_loss: 0.6598 - val_acc: 0.7807\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 16s 345ms/step - loss: 0.8630 - acc: 0.7216 - val_loss: 0.7300 - val_acc: 0.7389\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 15s 336ms/step - loss: 0.8321 - acc: 0.7303 - val_loss: 0.5343 - val_acc: 0.8191\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 16s 340ms/step - loss: 0.8008 - acc: 0.7410 - val_loss: 0.5583 - val_acc: 0.8103\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 17s 362ms/step - loss: 0.7652 - acc: 0.7543 - val_loss: 0.3227 - val_acc: 0.9064\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 16s 347ms/step - loss: 0.7432 - acc: 0.7614 - val_loss: 0.3924 - val_acc: 0.8838\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 16s 358ms/step - loss: 0.7142 - acc: 0.7722 - val_loss: 0.3276 - val_acc: 0.9001\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 15s 330ms/step - loss: 0.6795 - acc: 0.7834 - val_loss: 0.3457 - val_acc: 0.9068\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 16s 351ms/step - loss: 0.6494 - acc: 0.7934 - val_loss: 0.3281 - val_acc: 0.8978\n",
      "Epoch 14/200\n",
      "46/46 [==============================] - 15s 336ms/step - loss: 0.6331 - acc: 0.7992 - val_loss: 0.2692 - val_acc: 0.9244\n",
      "Epoch 15/200\n",
      "46/46 [==============================] - 16s 357ms/step - loss: 0.5953 - acc: 0.8111 - val_loss: 0.2854 - val_acc: 0.9177\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - 16s 349ms/step - loss: 0.5790 - acc: 0.8176 - val_loss: 0.2466 - val_acc: 0.9290\n",
      "Epoch 17/200\n",
      "46/46 [==============================] - 18s 385ms/step - loss: 0.5548 - acc: 0.8241 - val_loss: 0.4229 - val_acc: 0.8604\n",
      "Epoch 18/200\n",
      "46/46 [==============================] - 16s 351ms/step - loss: 0.5100 - acc: 0.8393 - val_loss: 0.2172 - val_acc: 0.9414\n",
      "Epoch 19/200\n",
      "46/46 [==============================] - 16s 349ms/step - loss: 0.4934 - acc: 0.8443 - val_loss: 0.2376 - val_acc: 0.9331\n",
      "Epoch 20/200\n",
      "46/46 [==============================] - 16s 348ms/step - loss: 0.4747 - acc: 0.8498 - val_loss: 0.4473 - val_acc: 0.8480\n",
      "Epoch 21/200\n",
      "46/46 [==============================] - 17s 364ms/step - loss: 0.4494 - acc: 0.8593 - val_loss: 0.3035 - val_acc: 0.9043\n",
      "Epoch 22/200\n",
      "46/46 [==============================] - 17s 365ms/step - loss: 0.4334 - acc: 0.8657 - val_loss: 0.2737 - val_acc: 0.9170\n",
      "Epoch 23/200\n",
      "46/46 [==============================] - 16s 343ms/step - loss: 0.4215 - acc: 0.8681 - val_loss: 0.1429 - val_acc: 0.9604\n",
      "Epoch 24/200\n",
      "46/46 [==============================] - 16s 356ms/step - loss: 0.4045 - acc: 0.8736 - val_loss: 0.1535 - val_acc: 0.9571\n",
      "Epoch 25/200\n",
      "46/46 [==============================] - 17s 363ms/step - loss: 0.3925 - acc: 0.8775 - val_loss: 0.1586 - val_acc: 0.9538\n",
      "Epoch 26/200\n",
      "46/46 [==============================] - 16s 343ms/step - loss: 0.3704 - acc: 0.8837 - val_loss: 0.1557 - val_acc: 0.9527\n",
      "Epoch 27/200\n",
      "46/46 [==============================] - 15s 335ms/step - loss: 0.3581 - acc: 0.8878 - val_loss: 0.1411 - val_acc: 0.9601\n",
      "Epoch 28/200\n",
      "46/46 [==============================] - 16s 352ms/step - loss: 0.3508 - acc: 0.8920 - val_loss: 0.1365 - val_acc: 0.9604\n",
      "Epoch 29/200\n",
      "46/46 [==============================] - 16s 346ms/step - loss: 0.3346 - acc: 0.8966 - val_loss: 0.1135 - val_acc: 0.9659\n",
      "Epoch 30/200\n",
      "46/46 [==============================] - 15s 336ms/step - loss: 0.3360 - acc: 0.8963 - val_loss: 0.1241 - val_acc: 0.9632\n",
      "Epoch 31/200\n",
      "46/46 [==============================] - 16s 351ms/step - loss: 0.3242 - acc: 0.8996 - val_loss: 0.1470 - val_acc: 0.9545\n",
      "Epoch 32/200\n",
      "46/46 [==============================] - 16s 351ms/step - loss: 0.3189 - acc: 0.9012 - val_loss: 0.1085 - val_acc: 0.9682\n",
      "Epoch 33/200\n",
      "46/46 [==============================] - 15s 327ms/step - loss: 0.3121 - acc: 0.9044 - val_loss: 0.1375 - val_acc: 0.9583\n",
      "Epoch 34/200\n",
      "46/46 [==============================] - 16s 352ms/step - loss: 0.3057 - acc: 0.9052 - val_loss: 0.1132 - val_acc: 0.9661\n",
      "Epoch 35/200\n",
      "46/46 [==============================] - 16s 343ms/step - loss: 0.2989 - acc: 0.9082 - val_loss: 0.1109 - val_acc: 0.9676\n",
      "Epoch 36/200\n",
      "46/46 [==============================] - 16s 350ms/step - loss: 0.2890 - acc: 0.9101 - val_loss: 0.0846 - val_acc: 0.9747\n",
      "Epoch 37/200\n",
      "46/46 [==============================] - 15s 333ms/step - loss: 0.2814 - acc: 0.9141 - val_loss: 0.1028 - val_acc: 0.9683\n",
      "Epoch 38/200\n",
      "46/46 [==============================] - 16s 350ms/step - loss: 0.2754 - acc: 0.9142 - val_loss: 0.1047 - val_acc: 0.9702\n",
      "Epoch 39/200\n",
      "46/46 [==============================] - 16s 357ms/step - loss: 0.2810 - acc: 0.9132 - val_loss: 0.0913 - val_acc: 0.9716\n",
      "Epoch 40/200\n",
      "46/46 [==============================] - 16s 342ms/step - loss: 0.2657 - acc: 0.9177 - val_loss: 0.0943 - val_acc: 0.9713\n",
      "Epoch 41/200\n",
      "46/46 [==============================] - 16s 347ms/step - loss: 0.2666 - acc: 0.9176 - val_loss: 0.1127 - val_acc: 0.9660\n",
      "Epoch 42/200\n",
      "46/46 [==============================] - 16s 354ms/step - loss: 0.2592 - acc: 0.9195 - val_loss: 0.0836 - val_acc: 0.9745\n",
      "Epoch 43/200\n",
      "46/46 [==============================] - 16s 356ms/step - loss: 0.2545 - acc: 0.9205 - val_loss: 0.0738 - val_acc: 0.9776\n",
      "Epoch 44/200\n",
      "46/46 [==============================] - 16s 345ms/step - loss: 0.2580 - acc: 0.9200 - val_loss: 0.0843 - val_acc: 0.9743\n",
      "Epoch 45/200\n",
      "46/46 [==============================] - 15s 329ms/step - loss: 0.2603 - acc: 0.9209 - val_loss: 0.0851 - val_acc: 0.9755\n",
      "Epoch 46/200\n",
      "46/46 [==============================] - 16s 344ms/step - loss: 0.2430 - acc: 0.9252 - val_loss: 0.0700 - val_acc: 0.9781\n",
      "Epoch 47/200\n",
      "46/46 [==============================] - 15s 332ms/step - loss: 0.2431 - acc: 0.9248 - val_loss: 0.0752 - val_acc: 0.9771\n",
      "Epoch 48/200\n",
      "46/46 [==============================] - 15s 330ms/step - loss: 0.2412 - acc: 0.9253 - val_loss: 0.0749 - val_acc: 0.9772\n",
      "Epoch 49/200\n",
      "46/46 [==============================] - 17s 367ms/step - loss: 0.2401 - acc: 0.9270 - val_loss: 0.0691 - val_acc: 0.9778\n",
      "Epoch 50/200\n",
      "46/46 [==============================] - 17s 365ms/step - loss: 0.2421 - acc: 0.9251 - val_loss: 0.0734 - val_acc: 0.9774\n",
      "Epoch 51/200\n",
      "46/46 [==============================] - 17s 366ms/step - loss: 0.2298 - acc: 0.9294 - val_loss: 0.0730 - val_acc: 0.9781\n",
      "Epoch 52/200\n",
      "46/46 [==============================] - 16s 350ms/step - loss: 0.2327 - acc: 0.9302 - val_loss: 0.0738 - val_acc: 0.9773\n",
      "Epoch 53/200\n",
      "46/46 [==============================] - 17s 372ms/step - loss: 0.2287 - acc: 0.9297 - val_loss: 0.0704 - val_acc: 0.9791\n",
      "Epoch 54/200\n",
      "46/46 [==============================] - 16s 339ms/step - loss: 0.2248 - acc: 0.9298 - val_loss: 0.0631 - val_acc: 0.9809\n",
      "Epoch 55/200\n",
      "46/46 [==============================] - 16s 352ms/step - loss: 0.2303 - acc: 0.9292 - val_loss: 0.0618 - val_acc: 0.9812\n",
      "Epoch 56/200\n",
      "46/46 [==============================] - 17s 360ms/step - loss: 0.2198 - acc: 0.9320 - val_loss: 0.0600 - val_acc: 0.9817\n",
      "Epoch 57/200\n",
      "46/46 [==============================] - 16s 356ms/step - loss: 0.2196 - acc: 0.9323 - val_loss: 0.0659 - val_acc: 0.9798\n",
      "Epoch 58/200\n",
      "46/46 [==============================] - 16s 345ms/step - loss: 0.2211 - acc: 0.9324 - val_loss: 0.0783 - val_acc: 0.9757\n",
      "Epoch 59/200\n",
      "46/46 [==============================] - 17s 377ms/step - loss: 0.2152 - acc: 0.9348 - val_loss: 0.0589 - val_acc: 0.9808\n",
      "Epoch 60/200\n",
      "46/46 [==============================] - 16s 351ms/step - loss: 0.2128 - acc: 0.9340 - val_loss: 0.0654 - val_acc: 0.9788\n",
      "Epoch 61/200\n",
      "46/46 [==============================] - 17s 373ms/step - loss: 0.2132 - acc: 0.9345 - val_loss: 0.0657 - val_acc: 0.9789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/200\n",
      "46/46 [==============================] - 16s 355ms/step - loss: 0.2099 - acc: 0.9350 - val_loss: 0.0637 - val_acc: 0.9787\n",
      "Epoch 63/200\n",
      "46/46 [==============================] - 16s 356ms/step - loss: 0.2083 - acc: 0.9357 - val_loss: 0.0661 - val_acc: 0.9786\n",
      "Epoch 64/200\n",
      "46/46 [==============================] - 17s 371ms/step - loss: 0.2080 - acc: 0.9365 - val_loss: 0.0576 - val_acc: 0.9820\n",
      "Epoch 65/200\n",
      "46/46 [==============================] - 17s 378ms/step - loss: 0.2019 - acc: 0.9376 - val_loss: 0.0661 - val_acc: 0.9786\n",
      "Epoch 66/200\n",
      "46/46 [==============================] - 17s 365ms/step - loss: 0.2039 - acc: 0.9369 - val_loss: 0.0791 - val_acc: 0.9774\n",
      "Epoch 67/200\n",
      "46/46 [==============================] - 17s 371ms/step - loss: 0.2045 - acc: 0.9370 - val_loss: 0.0562 - val_acc: 0.9818\n",
      "Epoch 68/200\n",
      "46/46 [==============================] - 16s 358ms/step - loss: 0.1984 - acc: 0.9397 - val_loss: 0.0539 - val_acc: 0.9818\n",
      "Epoch 69/200\n",
      "46/46 [==============================] - 15s 337ms/step - loss: 0.2004 - acc: 0.9388 - val_loss: 0.0503 - val_acc: 0.9834\n",
      "Epoch 70/200\n",
      "46/46 [==============================] - 16s 343ms/step - loss: 0.1993 - acc: 0.9388 - val_loss: 0.0626 - val_acc: 0.9803\n",
      "Epoch 71/200\n",
      "46/46 [==============================] - 16s 346ms/step - loss: 0.1959 - acc: 0.9404 - val_loss: 0.0575 - val_acc: 0.9814\n",
      "Epoch 72/200\n",
      "46/46 [==============================] - 16s 350ms/step - loss: 0.1967 - acc: 0.9396 - val_loss: 0.0509 - val_acc: 0.9837\n",
      "Epoch 73/200\n",
      "46/46 [==============================] - 17s 371ms/step - loss: 0.1982 - acc: 0.9400 - val_loss: 0.0686 - val_acc: 0.9778\n",
      "Epoch 74/200\n",
      "46/46 [==============================] - 15s 321ms/step - loss: 0.2005 - acc: 0.9386 - val_loss: 0.0541 - val_acc: 0.9825\n",
      "Epoch 75/200\n",
      "46/46 [==============================] - 16s 349ms/step - loss: 0.1922 - acc: 0.9404 - val_loss: 0.0785 - val_acc: 0.9759\n",
      "Epoch 76/200\n",
      "46/46 [==============================] - 15s 335ms/step - loss: 0.1925 - acc: 0.9406 - val_loss: 0.0609 - val_acc: 0.9810\n",
      "Epoch 77/200\n",
      "46/46 [==============================] - 17s 368ms/step - loss: 0.1878 - acc: 0.9424 - val_loss: 0.0634 - val_acc: 0.9811\n",
      "Epoch 78/200\n",
      "46/46 [==============================] - 15s 323ms/step - loss: 0.1900 - acc: 0.9432 - val_loss: 0.0547 - val_acc: 0.9834\n",
      "Epoch 79/200\n",
      "46/46 [==============================] - 17s 365ms/step - loss: 0.1943 - acc: 0.9396 - val_loss: 0.0507 - val_acc: 0.9840\n",
      "Epoch 80/200\n",
      "46/46 [==============================] - 17s 366ms/step - loss: 0.1935 - acc: 0.9401 - val_loss: 0.0611 - val_acc: 0.9796\n",
      "Epoch 81/200\n",
      "46/46 [==============================] - 17s 368ms/step - loss: 0.1886 - acc: 0.9427 - val_loss: 0.0654 - val_acc: 0.9805\n",
      "Epoch 82/200\n",
      "46/46 [==============================] - 17s 378ms/step - loss: 0.1854 - acc: 0.9435 - val_loss: 0.0555 - val_acc: 0.9823\n",
      "Epoch 83/200\n",
      "46/46 [==============================] - 16s 339ms/step - loss: 0.1820 - acc: 0.9454 - val_loss: 0.0508 - val_acc: 0.9839\n",
      "Epoch 84/200\n",
      "46/46 [==============================] - 16s 340ms/step - loss: 0.1800 - acc: 0.9447 - val_loss: 0.0539 - val_acc: 0.9826\n",
      "Epoch 85/200\n",
      "46/46 [==============================] - 16s 348ms/step - loss: 0.1793 - acc: 0.9450 - val_loss: 0.0539 - val_acc: 0.9840\n",
      "Epoch 86/200\n",
      "46/46 [==============================] - 17s 361ms/step - loss: 0.1830 - acc: 0.9438 - val_loss: 0.0464 - val_acc: 0.9859\n",
      "Epoch 87/200\n",
      "46/46 [==============================] - 17s 370ms/step - loss: 0.1805 - acc: 0.9456 - val_loss: 0.0598 - val_acc: 0.9806\n",
      "Epoch 88/200\n",
      "46/46 [==============================] - 16s 356ms/step - loss: 0.1813 - acc: 0.9455 - val_loss: 0.0534 - val_acc: 0.9833\n",
      "Epoch 89/200\n",
      "46/46 [==============================] - 16s 346ms/step - loss: 0.1741 - acc: 0.9469 - val_loss: 0.0598 - val_acc: 0.9809\n",
      "Epoch 90/200\n",
      "46/46 [==============================] - 17s 379ms/step - loss: 0.1807 - acc: 0.9443 - val_loss: 0.0676 - val_acc: 0.9783\n",
      "Epoch 91/200\n",
      "46/46 [==============================] - 16s 346ms/step - loss: 0.1756 - acc: 0.9469 - val_loss: 0.0493 - val_acc: 0.9846\n",
      "Epoch 92/200\n",
      "46/46 [==============================] - 15s 327ms/step - loss: 0.1774 - acc: 0.9462 - val_loss: 0.0625 - val_acc: 0.9813\n",
      "Epoch 93/200\n",
      "46/46 [==============================] - 16s 358ms/step - loss: 0.1739 - acc: 0.9487 - val_loss: 0.0494 - val_acc: 0.9848\n",
      "Epoch 94/200\n",
      "46/46 [==============================] - 15s 330ms/step - loss: 0.1736 - acc: 0.9470 - val_loss: 0.0641 - val_acc: 0.9805\n",
      "Epoch 95/200\n",
      "46/46 [==============================] - 16s 347ms/step - loss: 0.1745 - acc: 0.9475 - val_loss: 0.0637 - val_acc: 0.9816\n",
      "Epoch 96/200\n",
      "46/46 [==============================] - 15s 328ms/step - loss: 0.1752 - acc: 0.9470 - val_loss: 0.0549 - val_acc: 0.9834\n",
      "Epoch 97/200\n",
      "46/46 [==============================] - 16s 358ms/step - loss: 0.1697 - acc: 0.9487 - val_loss: 0.0630 - val_acc: 0.9811\n",
      "Epoch 98/200\n",
      "46/46 [==============================] - 16s 349ms/step - loss: 0.1671 - acc: 0.9492 - val_loss: 0.0591 - val_acc: 0.9830\n",
      "Epoch 99/200\n",
      "46/46 [==============================] - 17s 363ms/step - loss: 0.1736 - acc: 0.9469 - val_loss: 0.0540 - val_acc: 0.9837\n",
      "Epoch 100/200\n",
      "46/46 [==============================] - 17s 373ms/step - loss: 0.1753 - acc: 0.9468 - val_loss: 0.0526 - val_acc: 0.9838\n",
      "Epoch 101/200\n",
      "46/46 [==============================] - 18s 389ms/step - loss: 0.1688 - acc: 0.9504 - val_loss: 0.0616 - val_acc: 0.9813\n",
      "Epoch 102/200\n",
      "46/46 [==============================] - 17s 369ms/step - loss: 0.1739 - acc: 0.9475 - val_loss: 0.0550 - val_acc: 0.9823\n",
      "Epoch 103/200\n",
      "46/46 [==============================] - 18s 381ms/step - loss: 0.1685 - acc: 0.9486 - val_loss: 0.0493 - val_acc: 0.9846\n",
      "Epoch 104/200\n",
      "46/46 [==============================] - 17s 370ms/step - loss: 0.1653 - acc: 0.9495 - val_loss: 0.0498 - val_acc: 0.9847\n",
      "Epoch 105/200\n",
      "46/46 [==============================] - 17s 379ms/step - loss: 0.1669 - acc: 0.9479 - val_loss: 0.0557 - val_acc: 0.9830\n",
      "Epoch 106/200\n",
      "46/46 [==============================] - 17s 380ms/step - loss: 0.1714 - acc: 0.9477 - val_loss: 0.0588 - val_acc: 0.9828\n",
      "Epoch 107/200\n",
      "46/46 [==============================] - 16s 353ms/step - loss: 0.1664 - acc: 0.9500 - val_loss: 0.0555 - val_acc: 0.9835\n",
      "Epoch 108/200\n",
      "46/46 [==============================] - 16s 359ms/step - loss: 0.1667 - acc: 0.9491 - val_loss: 0.0559 - val_acc: 0.9824\n",
      "Epoch 109/200\n",
      "46/46 [==============================] - 17s 375ms/step - loss: 0.1625 - acc: 0.9511 - val_loss: 0.0528 - val_acc: 0.9846\n",
      "Epoch 110/200\n",
      "46/46 [==============================] - 18s 383ms/step - loss: 0.1682 - acc: 0.9491 - val_loss: 0.0531 - val_acc: 0.9839\n",
      "Epoch 111/200\n",
      "46/46 [==============================] - 16s 347ms/step - loss: 0.1662 - acc: 0.9505 - val_loss: 0.0508 - val_acc: 0.9843\n",
      "Epoch 112/200\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9483"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-891592347e7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_train_samples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0minitial_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m          )\n\u001b[1;32m     42\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1313\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2248\u001b[0m                                 \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2249\u001b[0m                                 \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2250\u001b[0;31m                                 max_queue_size=max_queue_size)\n\u001b[0m\u001b[1;32m   2251\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2252\u001b[0m                             \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   2397\u001b[0m                                      \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2398\u001b[0m                                      str(generator_output))\n\u001b[0;32m-> 2399\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2400\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2401\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1922\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1924\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1925\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 200\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train_onehot = keras.utils.to_categorical(y_train, num_classes)\n",
    "x_test_norm = x_test/255.\n",
    "y_test_onehot = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "    x_train, y_train_onehot, \n",
    "    batch_size=batch_size)\n",
    "\n",
    "validation_generator = test_datagen.flow(\n",
    "    x_test, y_test_onehot, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='selu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='selu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='selu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=validation_generator,\n",
    "          validation_steps=nb_train_samples // batch_size,\n",
    "                    initial_epoch = 0\n",
    "         )\n",
    "score = model.evaluate(validation_generator, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
